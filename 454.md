Feb 2

Irreative vs Recursive Navigation function

Pros n Cons of having the root name server to handle all requests

* caching
* can't cache all reqeusts, must need a smart performance handleing
* Alot of duplicate caching

A DNS client is called a resolver
- normally implemented as library s/w
- communitcates with one or more name servers
- simple req=reply protocal is used using UDP
- client implements timeout and resends query if necessary
- can configure resplves to contact an initial list of name servers

eg in some order of preferences
- the DNS architecture allows for iterative of recursive navigation 
- the resolver can specify which type of navigation is required when contracting a name server
  - but name server is not bound to follow this 
  - can have issues with tying up server threads
  - can pack multiple queries in a single msg (and can get multiple replies in a single msg)



Piper and Filters
####

Transport Protocal

What is a Transport porotcal
* Every network product follows a 5 layer 
* UDP and TCP is on transport layer
* Transport layer lets multiple applications use one network connection simultaneously

USER Datagram Protocol

Advanatesgs:
* Small packet sizes (Header size smaller)
* No connection to create and maintain
* More control over when data is sent out

Disadvantages
* Primitive error detection 
* When corrurption is detected it does not recover, but discard it
* No compensation for lost packets
* Packets can arrive out of order
* No congestion control

UDP is lightweight but unreliable

TCP
The reliable, connection based choice

Requires a three way handshake

Advantages
* Delivery acknowledgments
* Retransmissions (when a sender doesnt get a deliver acknowledgement it'll re send)
* In-order delivery (might be sent out of order, but TCP will rearrange)
* Congestion Control
 * Delays transmission when the network is congested
 * Error detection (checksum for IPV4 IPV6)

Disadantages
* Bigger header
* Data doesnt always get sent out immediaately
 * side effect of congestion control
* Bigget overhead
 * Retransmissions of packets, acknowledgement of packets (Not always a good thing)

 UDP is message-oriented
 * Application sends data in distinct chunkcs
 * See also: mail

 TCP is stream oriented
 * Used as a continuous flow of data
 * Split up in chuncks by TCP
 * See also: phone conversation

 For Text communication
 * since low bandwidth
 * and importance of the guaranteed deliver/in order delivery transmission TCP is favoured over UDP

 For downloading
 * TCP has in-order and resubmission
 * deliver acknowledgments

 For multimedia streaming
 UDP 
 * has less overhead
 * Send delay is undesirable but less delay
 * Data loss can be masked

 TCP:
 * Better when its overhead doesnt deteriorate perforamnce
 * Better secuity then UDP


 DNS lookuos
 * use UDP
 * no overhead

#3/2/2017

* You either have good isolation or good perfromance but not both (you can get a turing award if you do)
* Inconsistent Retrievals: Aka fuzzyreads

* Recall Atomicity all or nothing
* What if we abort T1 when we already r2[x] in T2
	* This is a dirty read, we read dirty data

T1  |---------|---------|-------| 
    Start    r1[x]    w1[x]    end

T2            |---------|-------|---------|
			            r2[x]   w2[x]

So we have to obviously abort T2 if T1 is aborting bc r2[x] is a dirty read
But what if we have T3

T1  |---------|---------|-------| 
    Start    r1[x]    w1[x]    end

T2            |---------|-------|---------|
			            r2[x]   w2[x]

T3            			|---------|-------|---------|
								  r3[x]

So now we have to abort T3 aswell, this is cascading abort

The problem is poor performance, we are doing work but nothing to show for it

(refer to slide to 6-59)

r1[x] w2[x]
r2[x] w1[x] // this will conflict, different thread locations reading from the same place, you 
r2[x] w2[x] // these 2 will not conflict because they smae thread
w1[x] w2[x] // conflicts ur writing twice
r1[x] r2[x] // doenst conflict, because thety are both reads

Serializable?: (A conflict equivalent serial history) / (Serial history to H1?)

Serial Order:

Precedence graph:
Based on the graph from 6-60

There will be an edge if there is a conflict

For H1 

T1 -> T2
T2 -> T3
T2 -> T1 //already exists redundant
T3 -> T1 

Aside:
Thm: No cycles means that hisory is seralizable

The serial ordering in this cases is T2->T3->T1 //A path that includes every notde

Aside:
Thm: topological sort


For H2

Since h2 is not conflict equivalnt and not serializable then a cycle must exist

There is a cycle //maybe there is more
w2[x] r3[x] //From T2 to T3
r3[y] w2[y] // form T3 to T2

What is Local Serializability, Global serializability

Read locks 
Write locks

Two phase locking (2PL)
T1: w1[x] -> release, write lock on X


Sometimes 
Strick2Pl === Rigorous 2pl (whichc is strong strict 2pl)

#3/7/2017

Deadlocks
* Circular weight, Im holding onto resources you want and your holding onto resources that I wank and none of us are willing to comper	mise
* At least one of the resources are held in unshared modez

Deadlock Managment 
6-79
* Prevention
* Avoidance 
* Detection of Recovery

Choosing transaction to abort during a deadlock
1. What if we choose the transaction that has done the elast amount of work
  * Least amount of wasted work (progress of transactions)
2. Cost of aborting trans
3. Abort trans in multiple (most) deadlocks

Time stamp ordering

In timestamp ordering (TO), two timestamp are associated with each data item X

1. read_ts(X): largest ts from among all timestamps of transactions that have successfully read X
2. write_ts(x) largest timestamp from among all timestamps of transactions that have successfully written X 

T-> ts(1) -> T1, T2 -> r2[x] w2[x]

			
rts(X) = 0 	T1 = r1[x]
wts(X) = 0  

rts(X) = 1 	T1 = r1[x], w1[x]
wts(X) = 0  

rts(X) = 1  T1 = r1[x], w1[x]
wts(X) = 1  T2 = r2[x] 

rts(X) = 2  T1 = r1[x], w1[x]  commit
wts(X) = 1  T2 = r2[x]  nothin commit

So in this case we are all good

-----------------
			
rts(X) = 0 	T1 = r1[x]
wts(X) = 0  

rts(X) = 1 	T1 = r1[x], w1[x]
wts(X) = 0  

rts(X) = 1  T1 = r1[x], w1[x]
wts(X) = 1  T2 = r2[x] 

rts(X) = 2  T1 = r1[x], w1[x],  w1[x]-> abort, abort T1
wts(X) = 1  T2 = r2[x],   
//Since we aborted T1, T2 needs to be aborted to avoid dirty read (cascarding abort)
// once thing we can do is keeping a dirty bit so when T2 read x, we know T1 wrote x and postpones until T1 commits and hten it proceeds to read an commit this avoids (cascarding abort)

MVTO
* Multi versionedtime ordering

Module 7- Replication

Why replicatie

Performance
* If you have  server that gets alot of request it becomes bottleneck and single point of error
* We need to add anotehr server to send of the request and balance the load
* we are replicating the servers thus we also need to replicate the data

Availability
* If one server fails you can still run another 

Data Correuption / Consistency
* If we read from a few servers and there are variances in result (2 severs said values is 1, 1 server saids value is 0)
we can probably trust the majority more [the 1 server is corrupt]

Brings data close to client
* Make servers close to client
* Decrease reponse time
* increases availability 

#3/9/2017


Importantings of writing in order, if you write to one replicate , you probably need to write to all repliates, and do a logical copy where order matters.

func x - apply w1[x], w2[x]
	- replicate of x - apply w1[x], w2[x]	
 	- replicate of x - apply w1[x], w2[x]
 	- replicate of x - apply w2[x], w1[x] //BAD wrong ordered write is dangerous


##Stict Consistency
* order of reading and writing
* Any read(x) returns a value corresponding ot the result of the most recent write(x)

func x
 - replicate of x, x1
 - replicate of x, x2

Assume we first ran
r1[x] then r2[x] then w1[x] then w2[x]

 - replicate of x, x1 - apply r1[x] w1[x]
 - replicate of x, x2 - apply r2[x] w2[x]

will appear as
  func x- apply r1[x] r2[x]  w1[x] w2[x] in timestamp order

 //See the global mapsystems holding the logical mapping and physical mapping
 // in reality this will be impossible, phyiscal clocks will be offsync and clashes in timestamp
 // Nice model but impractical. If you can provide Strict consistency you should aim for thim

===============
If we picked

func x- apply w2[x] r2[x]  w1[x] r1[x] in timestamp order

 - replicate of x, x1 - apply r1[x] w1[x]
 - replicate of x, x2 - apply r2[x] w2[x]
 
// this is bad idea in the replicating we are not writing before we are readin

===============

Lets pick

r1[x] w1[x] r2[x] w2[x] 
// makes sense because it preserves ordering with repect to each client
// We should call this **Sequentical Consistenc (SC)**, it makes sense to a client but does it make senbse to all clients
// another sequenticaly consistent order would be r1[x] r2[x] w1[x] r2[x] and r2[x] w2[x] r1[x] w1[x] and many more
// with  sequenticaly consistent any of these orders is okay unlike Stict Consistency which only allows one
// a practical was of scheduling operations whens you have replicates 


##Earger (Synchronous) replication

Atomicity = all of nothing
When everyone is ready to commit and we do all the commit at once

1. large number of messages (expensive)
2. long lived (you have to coordinate all of them)
  * The long lived a transaction is the more likely it will fail (ie if a message gets lost ur fucked)
  * Poor scalability
3. Failure 
  * poor performance


Lazy way *asynchronous (somescalled lazy master)

* Each transaction runs when available 
* Each transactions is short lived
* Breaking down the update schema to numerous of small transaction
* Performance is clearly better
* Problem is how we propagate updates, updates  of the same transaction may have ordering issues ie (T2 before T1) resulting in different values

Example of where it sucks
* Assumes run concurrently

* Say they are run both at the same time, and commit both at the same time

[copy (x)  master(y)]
T1 - r[x] w[y]
* Since we are writing y we need to first it on master y
* We will then propagate w1[y] onto the below site

[master(x)  copy(y)]
T2 - r[y] w[x]
* Since we are writing x we need to first it on master x 
* We will then propagate w2[x] onto the above site

The problem is not we have transactions applied in different orders because we are not applying the trasnaction x and y in that order beause we need to write y first before writing x

This could be solved if we have another site with all the master sites, which we can be used to ensure the order
* How ever this site will become terribly overloaded since everyone is going to check this site
* Multiple transactions will be looking at the same lock, so this probably only works well for low workloads


Monotonic Reads, Montonic writes, etc

##3/16/2016

Update Propagation

Subscription: -> push / pull (pub-sub systems)
1. Send ops
2. Send Changed Values
3. Send data copy

###Push vs Pull protocols 7-18


###Epidemic Protocol

* Provides eventual consistency 
* Tables that keeps track of what you have sent and whathavnt been sent

###Quorum Based Protool
Quorum: Having enough people to vode pn a major decision

majority (copies) quorum consensus protocol
V(total Votes) = 3
Vr = 2, Vw =2

Vr + Vw > V

//What this means there is always an overwrite of reaading and writinge votes from a server
//Serializing the readers and writers


On the fnal exam

what does this rule mean Vw >V/2
//you have to be done wiritng before i can write
//This rule seralizes writers 

Voting examples
* ROWA read one write all


#Module 8

* Dependability

##Failure Models

Crash Failure = fail stop failure

Hardware Redundency
* having a failover
* hotstandby / secondary standby


k-fault toleance
* Majority systems

Blue Army
- 3000

Red Army
- 5000
- stuby between the two factions of the blue army

Blue Army
- 3000

If the blue army attacks the red army unilaterally they will lose
if they attack at once they will defeat the red army
A blue army general contacts the other blue army general to ask "attack at down", the other blue army
"replies with yes"

But how does the second blue army know his message got there.
Well we can get the first blue army to send an ack, but then the first blue army doesnt know if his message got there

Can never reaching consensus, unsolvable probelm 


#3/21/2017

Making sure if a transaction commits, even if the memory is whiped out we can gaurunantee persistent stroage 
WAL = write ahead log

How do we garauntee recovery

on recoery
* Scan the log from tail to head
* make a list of transactions
 - commited
 - aborted 
 - inflights
* scan from head to tail


Imagine this scenario

T1: begin
T1: x, 99, 100
T2: begin
T1: commit
T2: y, 0, 1
T3: start
T2: abort
T3: z, 9, 10

inflight T3
Aborted T2
Commited T1

Since T2 never commited it aborted so we dont need it on persistent storage

##Two phase commit protocol 

Coordinators
* either the distiributed action commits everywhere or aborts everywhere
* Coordinator asks particpant (are you ready to commit)


Participant
* if it is will to commit then it will vote yes


If all the participants say yes, then we perform a global commit otherwise a global abort

And then all pratricipant returns a done message, and then the coorindator can release state

When abborting a single participant, the safest thing to do is unilateral abort

Keyboards

Blocking uncerntain participants who repoended yes to the prepare request
Termination Protocol


In the diagram for 2 phase commit i
The head is covering the work done

Note that the protocol notes in the text book are incorrect

##3/23/2017

Replica Management with Network Partioning
Quorum Protocols for Network Partitioning

every  site has one vote
V= 12 
Vc = 10
Va = 3


A B c d 
E f g h
i j k l


A,B ,E  are in one partion, the rest are in ther toher partion

the problem is since there are only 9 sites in the largest provider partions u can never get 10 votes, 
but you always abort because there is a abort requirement of 3 votes. (Ofc we get to choose the values for Vc and Va but the Va + Vc > V rule must hold)

Michael the Grad students topics 

Erasure Coded Storage Systems

Cloud Storage Stoarge

Traditional Model, Application have walk operations to  stored data
But in Distributed system we have a virtual file system
Cloud Storage System requires a Name Server to help locate (like a Binder)


Difference
* API changes
* Block level semantics 
  * Read. Write update, deleteblocks
* Utilization of commodity hardware
* Fault Tolerance

Fault Tolerance
* Different faults at different level, disk , server, server rack, cluster
* Typical requirement: survive r
* independent failures: r fault tolerant
* need to surivive failures at r diffent levels (simliar to K fault we talked about before)

TLDR:
* Ensure coded storage system save space, but create distirbuted access
* Intelligent access strategies can help mitigate the distribtued access costs
* Moving data can further improve by helping co-locate co-accessed  data